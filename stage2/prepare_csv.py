import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(threshold=np.nan)
import os
import pandas as pd
import random
import nibabel as nib
import time
import math
import cv2
import argparse
from Affine_3d import AffineTransformation_3d as af3d

parser = argparse.ArgumentParser()

parser.add_argument('--on_mask_combination_2_csv_train', default='/home/salight/PE/reduction-all/csvfiles/on_mask_combination_train.csv', type=str, metavar='tp1',
                    help='A csv which indicates all GT PEs on PE129 trainset')
parser.add_argument('--on_mask_challenge_train', default='/home/salight/PE/reduction-challenge/csvfiles/on_mask_challenge_train.csv', type=str, metavar='tp2',
                    help='A csv which indicates all GT PEs on challenge trainset')

parser.add_argument('--big_train_csv_combination_2', default='/home/salight/PE/reduction-all/csvfiles/pred-train.csv', type=str, metavar='csv1',
                    help='A csv which indicates all suspected PEs on PE129 trainset, generated by stage 1')
parser.add_argument('--big_train_csv_challenge', default='/home/salight/PE/reduction-challenge/csvfiles/challenge_train_before_process.csv', type=str, metavar='csv2',
                    help='A csv which indicates all suspected PEs on challenge trainset, generated by stage 1')

parser.add_argument('--nii_combination_2_path', default='/home/salight/PE/dataset_original/combination-3/nii', type=str, metavar='nii1',
                    help='path to nii label of PE129 dataset')
parser.add_argument('--nii_challenge_path', default='/home/salight/PE/dataset_original/challenge/nii', type=str, metavar='nii2',
                    help='path to nii label of challenge dataset')

parser.add_argument('--ct_combination_2_path', default='/home/salight/PE/dataset_original/combination-3/ct_numpy/ct_clean', type=str, metavar='ct1',
                    help='path to ct numpy of PE129 dataset')
parser.add_argument('--spacing_combination_2_path', default='/home/salight/PE/dataset_original/combination-3/ct_numpy/spacing', type=str, metavar='spacing1',
                    help='path to spacing numpy of PE129 dataset')

parser.add_argument('--ct_challenge_path', default='/home/salight/PE/dataset_original/challenge/pe_np_resampled/ct_clean', type=str, metavar='ct2',
                    help='path to ct numpy of challenge dataset')
parser.add_argument('--spacing_challenge_path', default='/home/salight/PE/dataset_original/challenge/pe_np_resampled/spacing', type=str, metavar='spacing2',
                    help='path to spacing numpy of challenge dataset')

parser.add_argument('--pred_challenge_csv_2', default='/home/salight/PE/reduction-challenge/csvfiles/challenge_test_before_process_new.csv', type=str, metavar='candidates',
                    help='candidates predicted by stage1')

parser.add_argument('--target_csv_pca', default='/home/salight/PE/reduction-challenge/new/train-pca-3x.csv', type=str, metavar='pca_candidates',
                    help='processed training candidates with pca')
parser.add_argument('--target_csv_no_pca', default='/home/salight/PE/reduction-challenge/new/train-1x.csv', type=str, metavar='nonpca_candidates',
                    help='processed training candidates without pca')
parser.add_argument('--target_prediction_challenge_csv', default='/home/salight/PE/reduction-challenge/new/prediction-25-new.csv', type=str, metavar='candidates_processed',
                    help='processed testing candidates with or without pca')



def get_2d_img(df,ct_combination_2_path,ct_challenge_path,idx, threshold, pca=True):
    # df = pd.read_csv(csv)
    name = df.iloc[idx,0].split('-')
    ct_name, size, n = name[0], int(name[4]), int(name[5])
    print(ct_name)
    if os.path.exists( os.path.join(ct_challenge_path,ct_name[:-4]+"_clean.npy") ):
        HUs = np.load(os.path.join(ct_challenge_path,ct_name[:-4]+"_clean.npy"))
    elif os.path.exists(os.path.join(ct_combination_2_path,ct_name[:-4]+"_clean.npy")):
        HUs = np.load(os.path.join(ct_combination_2_path,ct_name[:-4]+"_clean.npy"))

    z_center, y_center, x_center = df.iloc[idx,3], df.iloc[idx,2], df.iloc[idx,1]
    z_size, y_size, x_size = size, size, size

    if pca:
        af = af3d(HUs, (z_center, y_center, x_center), (z_size, y_size, x_size))
        u = af.crop_img_and_do_pca(threshold)
        if u is None:
            M = af.compute_matrix((z_center, y_center, x_center), u)
        else:
            new_u = get_new_u(u, n)
            M = af.compute_matrix((z_center, y_center, x_center), new_u)
        out = af.affine_transform(M)
        assert out.shape == (z_size, y_size, x_size)
    else:
        x_min, x_max = x_center - x_size // 2, x_center + x_size // 2 + 1
        y_min, y_max = y_center - y_size // 2, y_center + y_size // 2 + 1
        z_min, z_max = z_center - z_size // 2, z_center + z_size // 2 + 1
        out = HUs[0, z_min:z_max, y_min:y_max, x_min:x_max]
        assert out.shape == (z_size, y_size, x_size)
    mid = out.shape[0] // 2
    slice0 = cv2.resize(out[mid, :, :], dsize=(32, 32))
    slice1 = cv2.resize(out[:, mid, :], dsize=(32, 32))
    slice2 = cv2.resize(out[:, :, mid], dsize=(32, 32))
    slice0 = np.expand_dims(slice0, 2)
    slice1 = np.expand_dims(slice1, 2)
    slice2 = np.expand_dims(slice2, 2)
    out_2d = np.concatenate((slice0, slice1, slice2), axis=2)
    return out_2d

def generate_train_df(on_mask_combination_2_csv_train,on_mask_challenge_train,big_train_csv_combination_2,num_combination_2,
                      ct_combination_2_path,spacing_combination_2_path,big_train_csv_challenge,num_challenge,ct_challenge_path,spacing_challenge_path,
                      sizes=[15, 19, 25, 29, 35], rotations=3, shifts=4, negative_size=25):
    begin = time.time()
    df1 = pd.read_csv(on_mask_combination_2_csv_train)
    df1 = df1.loc[:, ['seriesuid', 'coordX', 'coordY', 'coordZ']]
    df1 = add_on_mask_and_augment(df1,nii_combination_2_path,spacing_combination_2_path,sizes,rotations,shifts,negative_size,ground_truth=True)
    print("done1",time.time()-begin)

    begin = time.time()
    df2 = pd.read_csv(on_mask_challenge_train)
    df2 = df2.loc[:, ['seriesuid', 'coordX', 'coordY', 'coordZ']]
    df2 = add_on_mask_and_augment(df2,nii_challenge_path,spacing_challenge_path,sizes,rotations,shifts,negative_size,ground_truth=True)
    print("done2",time.time()-begin)

    begin = time.time()
    df_combination_2 = pd.read_csv(big_train_csv_combination_2)
    df_combination_2 = generate_little_csv(df_combination_2, num_combination_2)
    df_combination_2 = df_combination_2.loc[:,['seriesuid', 'coordX', 'coordY', 'coordZ']]
    df_combination_2 = df_combination_2.reset_index(drop=True)
    df_combination_2 = remove_unreasonable_candidate_in_csv(ct_combination_2_path, df_combination_2, spacing_combination_2_path)
    df_combination_2 = df_combination_2.reset_index(drop=True)
    df_combination_2 = add_on_mask_and_augment(df_combination_2,nii_combination_2_path,spacing_combination_2_path,
                                               sizes,rotations,shifts,negative_size,ground_truth=False)
    print("done3",time.time()-begin)

    begin = time.time()
    df_challenge = pd.read_csv(big_train_csv_challenge)
    df_challenge = generate_little_csv(df_challenge, num_challenge)
    df_challenge = df_challenge.reset_index(drop=True)
    df_challenge = remove_unreasonable_candidate_in_csv(ct_challenge_path, df_challenge, spacing_challenge_path)
    df_challenge = df_challenge.reset_index(drop=True)
    df_challenge = df_challenge.loc[:,['seriesuid', 'coordX', 'coordY', 'coordZ']]
    df_challenge = add_on_mask_and_augment(df_challenge,nii_challenge_path,spacing_challenge_path,
                                           sizes,rotations,shifts,negative_size,ground_truth=False)
    print("done4",time.time()-begin)

    df_all = pd.concat([df1,df2,df_combination_2,df_challenge])
    df_all = df_all.reset_index(drop=True)
    return df_all

def generate_test_df(pred_challenge_csv,ct_challenge_path,spacing_challenge_path,nii_challenge_path,negative_size=25):
    # on_mask_challenge_train,big_train_csv_combination_2,num_combination_2,
    #                   ,big_train_csv_challenge,num_challenge,ct_challenge_path,spacing_challenge_path,
    #                   , negative_size=25):
    begin = time.time()
    df = pd.read_csv(pred_challenge_csv)
    df = remove_unreasonable_candidate_in_csv(ct_challenge_path, df, spacing_challenge_path)
    df = df.reset_index(drop=True)

    on_masks = []
    # new_df = pd.DataFrame(columns=('seriesuid', 'coordX', 'coordY', 'coordZ','probability','diameter_mm'))

    for i in range(len(df)):
        mask = nib.load(os.path.join(nii_challenge_path, df.iloc[i, 0][:-4] + '.nii.gz'))
        mask = mask.get_data()
        spacing = np.load(os.path.join(spacing_challenge_path, df.iloc[i, 0][:-4] + '_spacing.npy'))  # z,y,x
        on_mask = point_in_mask(mask, (int(df.iloc[i, 1]), int(df.iloc[i, 2]), int(df.iloc[i, 3])))
        on_masks.append(on_mask)

        new_x, new_y, new_z = int(df.iloc[i, 1]), int(df.iloc[i, 2]), int(df.iloc[i, 3])
        df.iloc[i, 0] = df.iloc[i, 0] + '-' + str(new_x) + '-' + str(new_y) + '-' + str(new_z) + '-' + str(negative_size) + '-' + str(0)
        x_resampled, y_resampled, z_resampled = to_resampled_label(spacing, (new_z, new_y, new_x))
        df.iloc[i, 1], df.iloc[i,2], df.iloc[i, 3] = x_resampled, y_resampled, z_resampled

    df['on_mask'] = on_masks
    print("done1",time.time()-begin)

    return df

def add_on_mask_and_augment(df,nii_path,spacing_path,sizes,rotations,shifts,negative_size,ground_truth=True):
    on_masks = []
    new_df = pd.DataFrame(columns=('seriesuid', 'coordX', 'coordY', 'coordZ'))

    for i in range(len(df)):
        mask = nib.load(os.path.join(nii_path,df.iloc[i,0][:-4]+'.nii.gz'))
        mask = mask.get_data() # x,y,z
        spacing = np.load(os.path.join(spacing_path, df.iloc[i,0][:-4] + '_spacing.npy'))   #z,y,x
        on_mask = point_in_mask(mask, (int(df.iloc[i,1]),int(df.iloc[i,2]),int(df.iloc[i,3])))
        if on_mask == 1:
            if not ground_truth:
                continue
            on_masks.extend([1]*len(sizes)*rotations*shifts)
            shift = 0
            while shift < shifts:
                if shift == 0:
                    x_offset, y_offset, z_offset = 0,0,0
                else:
                    x_offset, y_offset, z_offset = np.random.randint(-8, 8), np.random.randint(-8, 8), np.random.randint(-8,8)  #unresample
                new_x, new_y, new_z = int(df.iloc[i,1]) + x_offset, int(df.iloc[i,2]) + y_offset, int(df.iloc[i,3]) + z_offset  #unresample
                if not point_in_mask(mask, (int(new_x),int(new_y),int(new_z))):   #unresample
                    continue
                shift += 1
                for size in sizes:
                    for rotation in range(rotations):
                        x_resampled, y_resampled, z_resampled = to_resampled_label(spacing, (new_z,new_y,new_x))
                        row = [{'seriesuid': df.iloc[i, 0] + '-' + str(new_x) + '-' + str(new_y) + '-' + str(new_z) + '-' + str(size) + '-' + str(rotation),
                                'coordX': x_resampled, 'coordY': y_resampled,'coordZ': z_resampled}]  #003.npy-x-y-z-size-rotation  rotation = 0,1,2 when rotations == 3
                        new_df = new_df.append(row,ignore_index=True)

        else:
            if ground_truth:
                continue
            on_masks.append(0)
            x_resampled, y_resampled, z_resampled = to_resampled_label(spacing, (df.iloc[i,3], df.iloc[i,2], df.iloc[i,1]))
            row = [{'seriesuid': df.iloc[i, 0] + '-' + str(int(df.iloc[i,1])) + '-' + str(int(df.iloc[i,2])) + '-' + str(int(df.iloc[i,3])) + '-' + str(negative_size) + '-' + str(0),
                    'coordX': x_resampled, 'coordY': y_resampled, 'coordZ': z_resampled}]  # 003.npy-x-y-z-size-rotation    rotation==0 when rotations == 1
            new_df = new_df.append(row, ignore_index=True)
        print("done for ",df.iloc[i,0],int(df.iloc[i,1]),int(df.iloc[i,2]),int(df.iloc[i,3]))

    new_df['on_mask'] = on_masks
    return new_df

def point_in_mask(mask,point):
    """
    The point and the mask must be unresampled
    :param mask: numpy array (x,y,z) unresampled
    :return: 0 or 1
    """
    index0, index1, index2 = point   #(x,y,z)
    if mask[index0,index1,index2] > 0:
        return 1
    else:
        return 0

def rotation_matrix(axis, theta):
    """
    Return the rotation matrix associated with counterclockwise rotation about
    the given axis by theta radians.
    """
    axis = np.asarray(axis)
    theta = np.asarray(theta)
    axis = axis/math.sqrt(np.dot(axis, axis))
    a = math.cos(theta/2)
    b, c, d = -axis*math.sin(theta/2)
    aa, bb, cc, dd = a*a, b*b, c*c, d*d
    bc, ad, ac, ab, bd, cd = b*c, a*d, a*c, a*b, b*d, c*d
    return np.array([[aa+bb-cc-dd, 2*(bc+ad), 2*(bd-ac)],
                     [2*(bc-ad), aa+cc-bb-dd, 2*(cd+ab)],
                     [2*(bd+ac), 2*(cd-ab), aa+dd-bb-cc]])

def get_new_u(u,n):
    """
    :param u: three column vector in svd u
    :param n: theta = n * 2 * pi / 5
    :return: new_u
    """
    theta = n * 2 * np.pi / 5
    u2,u1,u0 = u[:, 2].reshape(3, 1), u[:, 1].reshape(3, 1), u[:, 0].reshape(3, 1)
    matrix = rotation_matrix(u0.reshape(3,), theta)   # (3,3)
    new_u1 = np.dot(matrix,u1)
    new_u2 = np.dot(matrix,u2)
    new_u = np.hstack((u0,new_u1,new_u2))
    return new_u

def remove_unreasonable_candidate_in_csv(ct_path,df,spacing_path):
    """
    This function aims to remove unreasonable(near the border) candidates
    :param ct_path: resampled numpy
    :param df: seriesuid,coordX,coordY,coordZ        with unresampled coordinates
    :param spacing_path: resample ratio
    :return: resonable df
    """
    indices = []
    for i in range(len(df)):
        patient_name = df.iloc[i, 0]
        spacing = np.load(os.path.join(spacing_path, patient_name[:-4] + '_spacing.npy'))
        HUs = np.load(os.path.join(ct_path, patient_name[:-4] + '_clean.npy'))  # [0,255] computed from [-1200,600]
        _, z_shape, y_shape, x_shape = HUs.shape
        x_origin, y_origin, z_origin = df.iloc[i, 1], df.iloc[i, 2], df.iloc[i, 3]
        x, y, z = to_resampled_label(spacing, (z_origin, y_origin, x_origin))
        if z > z_shape-25 or y > y_shape-25 or x > x_shape-25 or z < 25 or y < 25 or x < 25:
            print(i,patient_name,'has some problem')
            indices.append(i)
    df.drop(indices,inplace=True)
    to_df = df.reset_index(drop=True)
    return to_df

def to_resampled_label(spacing,point):
    """
    :param spacing: <1
    :param point: (z,y,x)
    :return: x,y,z
    """
    new_z,new_y,new_x = np.array(point) * spacing
    return int(new_x),int(new_y),int(new_z)

def generate_little_csv(df_big,sample_num):
    sample_list = random.sample(range(0,len(df_big)),sample_num)
    all_list = list(range(0,len(df_big)))
    drop_list = list(set(all_list) - set(sample_list))
    df_little = df_big.drop(drop_list)
    return df_little


def show_2D_img(img_save_path,path_2D):
    if not os.path.exists(img_save_path):
        os.mkdir(img_save_path)
    names = os.listdir(path_2D)
    names.sort()
    for name in names:
        # plt.figure()
        arr = np.load( os.path.join(path_2D,name) )
        cv2.imwrite( os.path.join(img_save_path,name + "-0.jpg"), arr[:, :, 0])
        cv2.imwrite( os.path.join(img_save_path,name + "-1.jpg"), arr[:, :, 1])
        cv2.imwrite( os.path.join(img_save_path,name + "-2.jpg"), arr[:, :, 2])
        print(name)

if __name__ == "__main__":
    args = parser.parse_args()
    # on_mask: ground truth
    on_mask_combination_2_csv_train = args.on_mask_combination_2_csv_train
    on_mask_challenge_train = args.on_mask_challenge_train

    # stage 1 candidates on training dataset
    big_train_csv_combination_2 = args.big_train_csv_combination_2
    big_train_csv_challenge = args.big_train_csv_challenge

    # nii.gz
    nii_combination_2_path = args.nii_combination_2_path
    nii_challenge_path = args.nii_challenge_path

    # predictions in challenge
    pred_challenge_csv_2 = args.pred_challenge_csv_2

    # this folder includes all the resampled ct data in hospital and public(PE129)
    ct_combination_2_path = args.ct_combination_2_path
    spacing_combination_2_path = args.spacing_combination_2_path

    # this folder includes all the resampled ct data in challenge
    ct_challenge_path = args.ct_challenge_path
    spacing_challenge_path = args.spacing_challenge_path

    target_csv_pca = args.target_csv_pca
    target_csv_no_pca = args.target_csv_no_pca
    target_prediction_challenge_csv = args.target_prediction_challenge_csv

    """
    The two numbers below are set based on the number of true PEs in the Ground Truth. After data augmentation on true PE, we keep the 
    ratio between positive samples and negative samples 1:1

    Notice that no augmentation is performed on test set and no augmentation is performed on true PEs in training set

    When using PCA, *3 means rotation around main eigen vector
    """

    num_combination_2 = 3900 *3
    num_challenge = 1100 * 3
    df_all = generate_train_df(on_mask_combination_2_csv_train, on_mask_challenge_train, big_train_csv_combination_2,
                      num_combination_2, ct_combination_2_path, spacing_combination_2_path, big_train_csv_challenge, num_challenge,
                      ct_challenge_path, spacing_challenge_path,rotations=3)
    df_all.to_csv(target_csv_pca,index=False)

    df_test = generate_test_df(pred_challenge_csv_2, ct_challenge_path, spacing_challenge_path, nii_challenge_path)
    df_test.to_csv(target_prediction_challenge_csv, index=False)
    
    # Not using PCA
    num_combination_2 = 3900
    num_challenge = 1100
    df_all = generate_train_df(on_mask_combination_2_csv_train, on_mask_challenge_train, big_train_csv_combination_2,
                               num_combination_2, ct_combination_2_path, spacing_combination_2_path,
                               big_train_csv_challenge, num_challenge,
                               ct_challenge_path, spacing_challenge_path,rotations=1)
    df_all.to_csv(target_csv_no_pca, index=False)

    """
    You can debug your results via the commented code to show your patches on images

    """
    
    # path_2D = '/home/salight/PE/reduction-challenge/new/np'
    # df = pd.read_csv('/home/salight/PE/reduction-challenge/new/train-1x.csv')
    # for idx in range(len(df)):
    #     threshold = 120
    #     out, name, label = get_2d_img(df, ct_combination_2_path, ct_challenge_path, idx, threshold, pca=False)
    #     np.save(os.path.join(path_2D,name),out)

    # img_save_path = '/home/salight/PE/reduction-challenge/new/img'
    # show_2D_img(img_save_path, path_2D)

    # path = '/home/salight/PE/reduction-challenge/new/img'
    # os.mkdir(path)
    # for idx in range(14939,14948):
    #     csv = '/home/salight/PE/reduction-challenge/new/train.csv'
    #     out2d = get_2d_img(csv, ct_combination_2_path, ct_challenge_path, idx,pca=True)
    #     cv2.imwrite(os.path.join(path, str(idx) + '-1.jpg'), out2d[:, :, 0])
    #     cv2.imwrite(os.path.join(path, str(idx) + '-2.jpg'), out2d[:, :, 1])
    #     cv2.imwrite(os.path.join(path, str(idx) + '-3.jpg'), out2d[:, :, 2])


